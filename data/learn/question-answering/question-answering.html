<!DOCTYPE html>
<html lang="en">

<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-WRJ69W24DT"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-WRJ69W24DT');
</script>

    <title>Question Answering (QA) Guide - Models, Systems & Resources - CS Archive</title>
    <meta name="description" content="A comprehensive guide to Question Answering, covering recent trends in QA models, systems, and resources, including tutorials, libraries, tools, and documentati">
    <meta name="keywords" content="question answering, qa, natural language processing, nlp, machine learning, deep learning, information retrieval, transformer models, dilbert, unifiedqa, proqa, tydi qa, retrospective reader, tanda, electra, tinybert, minilm, t5, ernie, xlnet, albert, roberta, distilbert, spanbert, bert">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Question Answering (QA) Guide - Models, Systems & Resources">
    <meta property="og:description" content="A comprehensive guide to Question Answering, covering recent trends in QA models, systems, and resources, including tutorials, libraries, tools, and documentati">
    <meta property="og:url" content="https://archive.dharunashokkumar.com/data/learn/question-answering/question-answering.html">
    <meta property="og:image" content="https://archive.dharunashokkumar.com/assets/og-image.png">
    <meta property="og:site_name" content="CS Archive">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Question Answering (QA) Guide - Models, Systems & Resources">
    <meta name="twitter:description" content="A comprehensive guide to Question Answering, covering recent trends in QA models, systems, and resources, including tutorials, libraries, tools, and documentati">
    <link rel="canonical" href="https://archive.dharunashokkumar.com/data/learn/question-answering/question-answering.html">


    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="../markdown-page.css">
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
</head>

<body>
    <!-- SEO: Static content for search engines -->
    <article id="content">
        <noscript>
            <h1>Awesome Question Answering <a href="https://github.com/sindresorhus/awesome"><img alt="Awesome" src="https://awesome.re/badge.svg" /></a></h1>
<p><em>A curated list of the __<a href="https://en.wikipedia.org/wiki/Question_answering">Question Answering (QA)</a>__ subject which is a computer science discipline within the fields of information retrieval and natural language processing (NLP) toward using machine learning and deep learning</em></p>
<p><em>정보 검색 및 자연 언어 처리 분야의 질의응답에 관한 큐레이션 - 머신러닝과 딥러닝 단계까지</em><br/>
<em>问答系统主题的精选列表，是信息检索和自然语言处理领域的计算机科学学科 - 使用机器学习和深度学习</em></p>
<h2>Contents</h2>
<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->

<ul>
<li><a href="#recent-trends">Recent Trends</a></li>
<li><a href="#about-qa">About QA</a></li>
<li><a href="#events">Events</a></li>
<li><a href="#systems">Systems</a></li>
<li><a href="#competitions-in-qa">Competitions in QA</a></li>
<li><a href="#publications">Publications</a></li>
<li><a href="#codes">Codes</a></li>
<li><a href="#lectures">Lectures</a></li>
<li><a href="#slides">Slides</a></li>
<li><a href="#dataset-collections">Dataset Collections</a></li>
<li><a href="#datasets">Datasets</a></li>
<li><a href="#books">Books</a></li>
<li><a href="#links">Links</a></li>
</ul>
<!-- END doctoc generated TOC please keep comment here to allow auto update -->

<h2>Recent Trends</h2>
<h3>Recent QA Models</h3>
<ul>
<li>DilBert: Delaying Interaction Layers in Transformer-based Encoders for Efficient Open Domain Question Answering (2020)</li>
<li>paper: https://arxiv.org/pdf/2010.08422.pdf</li>
<li>github: https://github.com/wissam-sib/dilbert</li>
<li>UnifiedQA: Crossing Format Boundaries With a Single QA System (2020)</li>
<li>Demo: https://unifiedqa.apps.allenai.org/</li>
<li>ProQA: Resource-efficient method for pretraining a dense corpus index for open-domain QA and IR. (2020)</li>
<li>paper: https://arxiv.org/pdf/2005.00038.pdf</li>
<li>github: https://github.com/xwhan/ProQA</li>
<li>TYDI QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages (2020)</li>
<li>paper: https://arxiv.org/ftp/arxiv/papers/2003/2003.05002.pdf</li>
<li>Retrospective Reader for Machine Reading Comprehension</li>
<li>paper: https://arxiv.org/pdf/2001.09694v2.pdf</li>
<li>TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection (AAAI 2020)</li>
<li>paper: https://arxiv.org/pdf/1911.04118.pdf</li>
</ul>
<h3>Recent Language Models</h3>
<ul>
<li><a href="https://openreview.net/pdf?id=r1xMH1BtvB">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a>, Kevin Clark, et al., ICLR, 2020.</li>
<li><a href="https://openreview.net/pdf?id=rJx0Q6EFPB">TinyBERT: Distilling BERT for Natural Language Understanding</a>, Xiaoqi Jiao, et al., ICLR, 2020.</li>
<li><a href="https://arxiv.org/abs/2002.10957">MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers</a>, Wenhui Wang, et al., arXiv, 2020.</li>
<li><a href="https://arxiv.org/abs/1910.10683">T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>, Colin Raffel, et al., arXiv preprint, 2019.</li>
<li><a href="https://arxiv.org/abs/1905.07129">ERNIE: Enhanced Language Representation with Informative Entities</a>, Zhengyan Zhang, et al., ACL, 2019.</li>
<li><a href="https://arxiv.org/abs/1906.08237">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a>, Zhilin Yang, et al., arXiv preprint, 2019.</li>
<li><a href="https://arxiv.org/abs/1909.11942">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a>, Zhenzhong Lan, et al., arXiv preprint, 2019.</li>
<li><a href="https://arxiv.org/abs/1907.11692">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>, Yinhan Liu, et al., arXiv preprint, 2019.</li>
<li><a href="https://arxiv.org/pdf/1910.01108.pdf">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a>, Victor sanh, et al., arXiv, 2019.</li>
<li><a href="https://arxiv.org/pdf/1907.10529v3.pdf">SpanBERT: Improving Pre-training by Representing and Predicting Spans</a>, Mandar Joshi, et al., TACL, 2019.</li>
<li><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>, Jacob Devlin, et al., NAACL 2019, 2018.</li>
</ul>
<h3>AAAI 2020</h3>
<ul>
<li><a href="https://arxiv.org/pdf/1911.04118.pdf">TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection</a>, Siddhant Garg, et al., AAAI 2020, Nov 2019.</li>
</ul>
<h3>ACL 2019</h3>
<ul>
<li><a href="https://www.aclweb.org/anthology/W19-5039">Overview of the MEDIQA 2019 Shared Task on Textual Inference,
Question Entailment and Question Answering</a>, Asma Ben Abacha, et al., ACL-W 2019, Aug 2019.</li>
<li><a href="https://arxiv.org/pdf/1906.02829v1.pdf">Towards Scalable and Reliable Capsule Networks for Challenging NLP Applications</a>, Wei Zhao, et al., ACL 2019, Jun 2019.</li>
<li><a href="https://arxiv.org/pdf/1905.05460v2.pdf">Cognitive Graph for Multi-Hop Reading Comprehension at Scale</a>, Ming Ding, et al., ACL 2019, Jun 2019.</li>
<li><a href="https://arxiv.org/abs/1906.05807">Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index</a>, Minjoon Seo, et al., ACL 2019, Jun 2019.</li>
<li><a href="https://arxiv.org/abs/1906.04980">Unsupervised Question Answering by Cloze Translation</a>, Patrick Lewis, et al., ACL 2019, Jun 2019.</li>
<li><a href="https://www.aclweb.org/anthology/S19-2153">SemEval-2019 Task 10: Math Question Answering</a>, Mark Hopkins, et al., ACL-W 2019, Jun 2019.</li>
<li><a href="https://arxiv.org/abs/1905.07098">Improving Question Answering over Incomplete KBs with Knowledge-Aware Reader</a>, Wenhan Xiong, et al., ACL 2019, May 2019.</li>
</ul>
        </noscript>
        <div id="dynamic-content">
            <h1>Awesome Question Answering <a href="https://github.com/sindresorhus/awesome"><img alt="Awesome" src="https://awesome.re/badge.svg" /></a></h1>
<p><em>A curated list of the __<a href="https://en.wikipedia.org/wiki/Question_answering">Question Answering (QA)</a>__ subject which is a computer science discipline within the fields of information retrieval and natural language processing (NLP) toward using machine learning and deep learning</em></p>
<p><em>정보 검색 및 자연 언어 처리 분야의 질의응답에 관한 큐레이션 - 머신러닝과 딥러닝 단계까지</em><br/>
<em>问答系统主题的精选列表，是信息检索和自然语言处理领域的计算机科学学科 - 使用机器学习和深度学习</em></p>
<h2>Contents</h2>
<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->

<ul>
<li><a href="#recent-trends">Recent Trends</a></li>
<li><a href="#about-qa">About QA</a></li>
<li><a href="#events">Events</a></li>
<li><a href="#systems">Systems</a></li>
<li><a href="#competitions-in-qa">Competitions in QA</a></li>
<li><a href="#publications">Publications</a></li>
<li><a href="#codes">Codes</a></li>
<li><a href="#lectures">Lectures</a></li>
<li><a href="#slides">Slides</a></li>
<li><a href="#dataset-collections">Dataset Collections</a></li>
<li><a href="#datasets">Datasets</a></li>
<li><a href="#books">Books</a></li>
<li><a href="#links">Links</a></li>
</ul>
<!-- END doctoc generated TOC please keep comment here to allow auto update -->

<h2>Recent Trends</h2>
<h3>Recent QA Models</h3>
<ul>
<li>DilBert: Delaying Interaction Layers in Transformer-based Encoders for Efficient Open Domain Question Answering (2020)</li>
<li>paper: https://arxiv.org/pdf/2010.08422.pdf</li>
<li>github: https://github.com/wissam-sib/dilbert</li>
<li>UnifiedQA: Crossing Format Boundaries With a Single QA System (2020)</li>
<li>Demo: https://unifiedqa.apps.allenai.org/</li>
<li>ProQA: Resource-efficient method for pretraining a dense corpus index for open-domain QA and IR. (2020)</li>
<li>paper: https://arxiv.org/pdf/2005.00038.pdf</li>
<li>github: https://github.com/xwhan/ProQA</li>
<li>TYDI QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages (2020)</li>
<li>paper: https://arxiv.org/ftp/arxiv/papers/2003/2003.05002.pdf</li>
<li>Retrospective Reader for Machine Reading Comprehension</li>
<li>paper: https://arxiv.org/pdf/2001.09694v2.pdf</li>
<li>TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection (AAAI 2020)</li>
<li>paper: https://arxiv.org/pdf/1911.04118.pdf</li>
</ul>
<h3>Recent Language Models</h3>
<ul>
<li><a href="https://openreview.net/pdf?id=r1xMH1BtvB">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a>, Kevin Clark, et al., ICLR, 2020.</li>
<li><a href="https://openreview.net/pdf?id=rJx0Q6EFPB">TinyBERT: Distilling BERT for Natural Language Understanding</a>, Xiaoqi Jiao, et al., ICLR, 2020.</li>
<li><a href="https://arxiv.org/abs/2002.10957">MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers</a>, Wenhui Wang, et al., arXiv, 2020.</li>
<li><a href="https://arxiv.org/abs/1910.10683">T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>, Colin Raffel, et al., arXiv preprint, 2019.</li>
<li><a href="https://arxiv.org/abs/1905.07129">ERNIE: Enhanced Language Representation with Informative Entities</a>, Zhengyan Zhang, et al., ACL, 2019.</li>
<li><a href="https://arxiv.org/abs/1906.08237">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a>, Zhilin Yang, et al., arXiv preprint, 2019.</li>
<li><a href="https://arxiv.org/abs/1909.11942">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a>, Zhenzhong Lan, et al., arXiv preprint, 2019.</li>
<li><a href="https://arxiv.org/abs/1907.11692">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>, Yinhan Liu, et al., arXiv preprint, 2019.</li>
<li><a href="https://arxiv.org/pdf/1910.01108.pdf">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a>, Victor sanh, et al., arXiv, 2019.</li>
<li><a href="https://arxiv.org/pdf/1907.10529v3.pdf">SpanBERT: Improving Pre-training by Representing and Predicting Spans</a>, Mandar Joshi, et al., TACL, 2019.</li>
<li><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>, Jacob Devlin, et al., NAACL 2019, 2018.</li>
</ul>
<h3>AAAI 2020</h3>
<ul>
<li><a href="https://arxiv.org/pdf/1911.04118.pdf">TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection</a>, Siddhant Garg, et al., AAAI 2020, Nov 2019.</li>
</ul>
<h3>ACL 2019</h3>
<ul>
<li><a href="https://www.aclweb.org/anthology/W19-5039">Overview of the MEDIQA 2019 Shared Task on Textual Inference,
Question Entailment and Question Answering</a>, Asma Ben Abacha, et al., ACL-W 2019, Aug 2019.</li>
<li><a href="https://arxiv.org/pdf/1906.02829v1.pdf">Towards Scalable and Reliable Capsule Networks for Challenging NLP Applications</a>, Wei Zhao, et al., ACL 2019, Jun 2019.</li>
<li><a href="https://arxiv.org/pdf/1905.05460v2.pdf">Cognitive Graph for Multi-Hop Reading Comprehension at Scale</a>, Ming Ding, et al., ACL 2019, Jun 2019.</li>
<li><a href="https://arxiv.org/abs/1906.05807">Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index</a>, Minjoon Seo, et al., ACL 2019, Jun 2019.</li>
<li><a href="https://arxiv.org/abs/1906.04980">Unsupervised Question Answering by Cloze Translation</a>, Patrick Lewis, et al., ACL 2019, Jun 2019.</li>
<li><a href="https://www.aclweb.org/anthology/S19-2153">SemEval-2019 Task 10: Math Question Answering</a>, Mark Hopkins, et al., ACL-W 2019, Jun 2019.</li>
<li><a href="https://arxiv.org/abs/1905.07098">Improving Question Answering over Incomplete KBs with Knowledge-Aware Reader</a>, Wenhan Xiong, et al., ACL 2019, May 2019.</li>
</ul>
        </div>
    </article>

    <script>
        // Load full markdown dynamically for users with JS
        const markdownFile = 'question-answering_README.md';

        fetch(markdownFile)
            .then(response => {
                if (!response.ok) throw new Error('File not found');
                return response.text();
            })
            .then(markdown => {
                document.getElementById('dynamic-content').innerHTML = marked.parse(markdown);
            })
            .catch(err => {
                console.error('Error loading markdown:', err);
                // Static content already visible, no action needed
            });
    </script>
</body>

</html>