<!DOCTYPE html>
<html lang="en">

<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-WRJ69W24DT"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-WRJ69W24DT');
</script>

    <title>Vision Language Models (VLMs) Architectures and Implementations - CS Archive</title>
    <meta name="description" content="Explore Vision Language Models (VLMs), their architectures, training procedures, and datasets. Find tutorials, libraries, tools, and resources for VLM developme">
    <meta name="keywords" content="Vision-Language-Models,VLMs,Architectures,Training-Procedures,Datasets,Tutorials,Libraries,Tools,Resources,Programming,Development,Guide,Documentation,Multimodal-Modeling,Cross-Attention,Masked-Language-Modeling,Image-Text-Matching">
    <meta name="robots" content="index, follow">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Vision Language Models (VLMs) Architectures and Implementations">
    <meta property="og:description" content="Explore Vision Language Models (VLMs), their architectures, training procedures, and datasets. Find tutorials, libraries, tools, and resources for VLM developme">
    <meta property="og:url" content="https://archive.dharunashokkumar.com/data/learn/vlm-architectures/vlm-architectures.html">
    <meta property="og:image" content="https://archive.dharunashokkumar.com/assets/og-image.png">
    <meta property="og:site_name" content="CS Archive">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Vision Language Models (VLMs) Architectures and Implementations">
    <meta name="twitter:description" content="Explore Vision Language Models (VLMs), their architectures, training procedures, and datasets. Find tutorials, libraries, tools, and resources for VLM developme">
    <link rel="canonical" href="https://archive.dharunashokkumar.com/data/learn/vlm-architectures/vlm-architectures.html">


    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="../markdown-page.css">
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
</head>

<body>
    <!-- SEO: Static content for search engines -->
    <article id="content">
        <noscript>
            <h1>üëÅÔ∏è‚Äçüó®Ô∏èAwesome VLM Architectures <a href="https://awesome.re"><img alt="Awesome" src="https://awesome.re/badge.svg" /></a></h1>
<p><img alt="VLM" src="https://github.com/gokayfem/Awesome-VLM-Architectures/assets/88277926/5c9ee091-1f37-4d92-8398-a7d4e006c014" /></p>
<p><strong>Vision-Language Models (VLMs)</strong> feature a multimodal architecture that processes image and text data simultaneously. They can perform <strong>Visual Question Answering (VQA)</strong>, <strong>image captioning</strong> and <strong>Text-To-Image search</strong> kind of tasks. VLMs utilize techniques like multimodal fusing with cross-attention, masked-language modeling, and image-text matching to relate visual semantics to textual representations. This repository contains information on famous Vision Language Models (VLMs), including details about their architectures, training procedures, and the datasets used for training. <strong>Click to expand for further details for every architecture</strong>
- üìô <a href="https://github.com/gokayfem/ComfyUI_VLM_nodes">Visit my other repo to try Vision Language Models on ComfyUI</a></p>
<h2>Contents</h2>
<ul>
<li><a href="#architectures">Architectures</a></li>
<li><a href="#important-references">Important References</a></li>
</ul>
<h2>Models</h2>
<p><a href="#llava-large-language-and-vision-assistant---visual-instruction-tuning">LLaVA</a> | <a href="#llava-15-improved-baselines-with-visual-instruction-tuning">LLaVA 1.5</a> | <a href="#llava-16-llava-next-improved-reasoning-ocr-and-world-knowledge">LLaVA 1.6</a> | <a href="#paligemma-a-versatile-and-transferable-3b-vision-language-model">PaliGemma</a> | <a href="#paligemma-2-a-family-of-versatile-vlms-for-transfer">PaliGemma 2</a> | <a href="#aimv2-multimodal-autoregressive-pre-training-of-large-vision-encoders">AIMv2</a> | <a href="#apollo-an-exploration-of-video-understanding-in-large-multimodal-models">Apollo</a> | <a href="#aria-an-open-multimodal-native-mixture-of-experts-model">ARIA</a> | <a href="#eve-unveiling-encoder-free-vision-language-models">EVE</a> | <a href="#evev2-improved-baselines-for-encoder-free-vision-language-models">EVEv2</a> | <a href="#janus-pro-unified-multimodal-understanding-and-generation-with-data-and-model-scaling">Janus-Pro</a> | <a href="#llava-cot-let-vision-language-models-reason-step-by-step">LLaVA-CoT</a> | <a href="#llm2clip-powerful-language-model-unlocks-richer-visual-representation">LLM2CLIP</a> | <a href="#maya-an-instruction-finetuned-multilingual-multimodal-model">Maya</a> | <a href="#minimax-01-scaling-foundation-models-with-lightning-attention">MiniMax-01</a> | <a href="#nvlm-open-frontier-class-multimodal-llms">NVLM</a> | <a href="#omnivlm-a-token-compressed-sub-billion-parameter-vision-language-model-for-efficient-on-device-inference">OmniVLM</a> | <a href="#pixtral-12b-a-cutting-edge-open-multimodal-language-model">Pixtral 12B</a> | <a href="#sa2va-marrying-sam2-with-llava-for-dense-grounded-understanding-of-images-and-videos">Sa2VA</a> | <a href="#tarsier2-advancing-large-vision-language-models-from-detailed-video-description-to-comprehensive-video-understanding">Tarsier2</a> | <a href="#ui-tars-pioneering-automated-gui-interaction-with-native-agents">UI-TARS</a> | <a href="#videochat-flash-hierarchical-compression-for-long-context-video-modeling">VideoChat-Flash</a> | <a href="#videollama-3-frontier-multimodal-foundation-models-for-image-and-video-understanding">VideoLLaMA 3</a> | <a href="#llama-32-vision-enhanced-multimodal-capabilities-built-on-llama-3">Llama 3.2-Vision</a> | <a href="#smolvlm-a-small-efficient-and-open-source-vision-language-model">SmolVLM</a> | <a href="#idefics">IDEFICS</a> | <a href="#idefics2">IDEFICS2</a> | <a href="#idefics3-8b-building-and-better-understanding-vision-language-models">IDEFICS3-8B</a> | <a href="#internlm-xcomposer2-mastering-free-form-text-image-composition-and-comprehension-in-vision-language-large-model">InternLM-XComposer2</a> | <a href="#internlm-xcomposer2-4khd-a-pioneering-large-vision-language-model-handling-resolutions-from-336-pixels-to-4k-hd">InternLM-XComposer2-4KHD</a> | <a href="#internlm-xcomposer-25-a-versatile-large-vision-language-model-supporting-long-contextual-input-and-output">InternLM-XComposer-2.5</a> | <a href="#internvl-25-expanding-performance-boundaries-of-open-source-multimodal-models-with-model-data-and-test-time-scaling">InternVL 2.5</a> | <a href="#deepseek-vl-towards-real-world-vision-language-understanding">DeepSeek-VL</a> | <a href="#deepseek-vl2-mixture-of-experts-vision-language-models-for-advanced-multimodal-understanding">DeepSeek-VL2</a> | <a href="#mantis-mastering-multi-image-understanding-through-interleaved-instruction-tuning">MANTIS</a> | <a href="#qwen-vl-a-versatile-vision-language-model-for-understanding-localization-text-reading-and-beyond">Qwen-VL</a> | <a href="#qwen2-vl-a-powerful-open-source-vision-language-model-for-image-and-video-understanding">Qwen2-VL</a> | <a href="#qwen25-vl-enhanced-vision-language-capabilities-in-the-qwen-series">Qwen2.5-VL</a> | <a href="#moondream1-and-moondream2">moondream1</a> | <a href="#moondream1-and-moondream2">moondream2</a> | <a href="#moondream-next-compact-vision-language-model-with-enhanced-capabilities">Moondream-next</a> | <a href="#sphinx-x-scaling-data-and-parameters-for-a-family-of-multi-modal-large-language-models">SPHINX-X</a> | <a href="#blip-bootstrapping-language-image-pre-training">BLIP</a> | <a href="#blip-2-bootstrapping-language-image-pre-training-with-frozen-image-encoders-and-large-language-models">BLIP-2</a> | <a href="#xgen-mm-blip-3-an-open-source-framework-for-building-powerful-and-responsible-large-multimodal-models">xGen-MM (BLIP-3)</a> | <a href="#instructblip-towards-general-purpose-vision-language-models-with-instruction-tuning">InstructBLIP</a> | <a href="#kosmos-1-language-is-not-all-you-need-aligning-perception-with-language-models">KOSMOS-1</a> | <a href="#kosmos-2-grounding-multimodal-large-language-models-to-the-world">KOSMOS-2</a> | <a href="#convllava-hierarchical-backbones-as-visual-encoder-for-large-multimodal-models">ConvLLaVA</a> | <a href="#parrot-multilingual-visual-instruction-tuning">Parrot</a> | <a href="#omg-llava-bridging-image-level-object-level-pixel-level-reasoning-and-understanding">OMG-LLaVA</a> | <a href="#evlm-an-efficient-vision-language-model-for-visual-understanding">EVLM</a> | <a href="#slowfast-llava-a-strong-training-free-baseline-for-video-large-language-models">SlowFast-LLaVA</a> | <a href="#nous-hermes-2-vision---mistral-7b">Nous-Hermes-2-Vision - Mistral 7B</a> | <a href="#tinygpt-v-efficient-multimodal-large-language-model-via-small-backbones">TinyGPT-V</a> | <a href="#covlm-composing-visual-entities-and-relationships-in-large-language-models-via-communicative-decoding">CoVLM</a> | <a href="#glamm-pixel-grounding-large-multimodal-model">GLaMM</a> | <a href="#cosmo-contrastive-streamlined-multimodal-model-with-interleaved-pre-training">COSMO</a> | <a href="#firellava">FireLLaVA</a> | <a href="#u-llava-unifying-multi-modal-tasks-via-large-language-model">u-LLaVA</a> | <a href="#moe-llava-mixture-of-experts-for-large-vision-language-models">MoE-LLaVA</a> | <a href="#bliva-a-simple-multimodal-llm-for-better-handling-of-text-rich-visual-questions">BLIVA</a> | <a href="#mobilevlm-a-fast-strong-and-open-vision-language-assistant-for-mobile-devices">MobileVLM</a> | <a href="#frozen-multimodal-few-shot-learning-with-frozen-language-models">FROZEN</a> | <a href="#flamingo-a-visual-language-model-for-few-shot-learning">Flamingo</a> | <a href="#openflamingo-an-open-source-framework-for-training-large-autoregressive-vision-language-models">OpenFlamingo</a> | <a href="#pali-a-jointly-scaled-multilingual-language-image-model">PaLI</a> | <a href="#pali-3-vision-language-models-smaller-faster-stronger">PaLI-3</a> | <a href="#palm-e-an-embodied-multimodal-language-model">PaLM-E</a> | <a href="#minigpt-4-enhancing-vision-language-understanding-with-advanced-large-language-models">MiniGPT-4</a> | <a href="#minigpt-v2-large-language-model-as-a-unified-interface-for-vision-language-multi-task-learning">MiniGPT-v2</a> | <a href="#llava-plus-learning-to-use-tools-for-creating-multimodal-agents">LLaVA-Plus</a> | <a href="#bakllava">BakLLaVA</a> | <a href="#cogvlm-visual-expert-for-pretrained-language-models">CogVLM</a> | <a href="#cogvlm2-enhanced-vision-language-models-for-image-and-video-understanding">CogVLM2</a> | <a href="#ferret-refer-and-ground-anything-anywhere-at-any-granularity">Ferret</a> | <a href="#fuyu-8b-a-multimodal-architecture-for-ai-agents">Fuyu-8B</a> | <a href="#otterhd-a-high-resolution-multi-modality-model">OtterHD</a> | <a href="#sphinx-the-joint-mixing-of-weights-tasks-and-visual-embeddings-for-multi-modal-large-language-models">SPHINX</a> | <a href="#eagle-2-building-post-training-data-strategies-from-scratch-for-frontier-vision-language-models">Eagle 2</a> | <a href="#eagle-exploring-the-design-space-for-multimodal-llms-with-mixture-of-encoders">EAGLE</a> | <a href="#vita-towards-open-source-interactive-omni-multimodal-llm">VITA</a> | <a href="#llava-onevision-easy-visual-task-transfer">LLaVA-OneVision</a> | <a href="#minicpm-o-26-a-gpt-4o-level-mllm-for-vision-speech-and-multimodal-live-streaming">MiniCPM-o-2.6</a> | <a href="#minicpm-v-a-gpt-4v-level-mllm-on-your-phone">MiniCPM-V</a> | <a href="#inf-llava-high-resolution-image-perception-for-multimodal-large-language-models">INF-LLaVA</a> | <a href="#florence-2-a-deep-dive-into-its-unified-architecture-and-multi-task-capabilities">Florence-2</a> | <a href="#multiinstruct-improving-multi-modal-zero-shot-learning-via-instruction-tuning">MULTIINSTRUCT</a> | <a href="#mousi-poly-visual-expert-vision-language-models">MouSi</a> | <a href="#lavin-cheap-and-quick-efficient-vision-language-instruction-tuning-for-large-language-models">LaVIN</a> | <a href="#clip-contrastive-language-image-pre-training">CLIP</a> | <a href="#metaclip-demystifying-clip-data">MetaCLIP</a> | <a href="#alpha-clip-a-clip-model-focusing-on-wherever-you-want">Alpha-CLIP</a> | <a href="#glip-grounded-language-image-pre-training">GLIP</a> | <a href="#imagebind-one-embedding-space-to-bind-them-all">ImageBind</a> | <a href="#siglip-sigmoid-loss-for-language-image-pre-training">SigLIP</a> | <a href="#vit-an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale">ViT</a> </p>
        </noscript>
        <div id="dynamic-content">
            <h1>üëÅÔ∏è‚Äçüó®Ô∏èAwesome VLM Architectures <a href="https://awesome.re"><img alt="Awesome" src="https://awesome.re/badge.svg" /></a></h1>
<p><img alt="VLM" src="https://github.com/gokayfem/Awesome-VLM-Architectures/assets/88277926/5c9ee091-1f37-4d92-8398-a7d4e006c014" /></p>
<p><strong>Vision-Language Models (VLMs)</strong> feature a multimodal architecture that processes image and text data simultaneously. They can perform <strong>Visual Question Answering (VQA)</strong>, <strong>image captioning</strong> and <strong>Text-To-Image search</strong> kind of tasks. VLMs utilize techniques like multimodal fusing with cross-attention, masked-language modeling, and image-text matching to relate visual semantics to textual representations. This repository contains information on famous Vision Language Models (VLMs), including details about their architectures, training procedures, and the datasets used for training. <strong>Click to expand for further details for every architecture</strong>
- üìô <a href="https://github.com/gokayfem/ComfyUI_VLM_nodes">Visit my other repo to try Vision Language Models on ComfyUI</a></p>
<h2>Contents</h2>
<ul>
<li><a href="#architectures">Architectures</a></li>
<li><a href="#important-references">Important References</a></li>
</ul>
<h2>Models</h2>
<p><a href="#llava-large-language-and-vision-assistant---visual-instruction-tuning">LLaVA</a> | <a href="#llava-15-improved-baselines-with-visual-instruction-tuning">LLaVA 1.5</a> | <a href="#llava-16-llava-next-improved-reasoning-ocr-and-world-knowledge">LLaVA 1.6</a> | <a href="#paligemma-a-versatile-and-transferable-3b-vision-language-model">PaliGemma</a> | <a href="#paligemma-2-a-family-of-versatile-vlms-for-transfer">PaliGemma 2</a> | <a href="#aimv2-multimodal-autoregressive-pre-training-of-large-vision-encoders">AIMv2</a> | <a href="#apollo-an-exploration-of-video-understanding-in-large-multimodal-models">Apollo</a> | <a href="#aria-an-open-multimodal-native-mixture-of-experts-model">ARIA</a> | <a href="#eve-unveiling-encoder-free-vision-language-models">EVE</a> | <a href="#evev2-improved-baselines-for-encoder-free-vision-language-models">EVEv2</a> | <a href="#janus-pro-unified-multimodal-understanding-and-generation-with-data-and-model-scaling">Janus-Pro</a> | <a href="#llava-cot-let-vision-language-models-reason-step-by-step">LLaVA-CoT</a> | <a href="#llm2clip-powerful-language-model-unlocks-richer-visual-representation">LLM2CLIP</a> | <a href="#maya-an-instruction-finetuned-multilingual-multimodal-model">Maya</a> | <a href="#minimax-01-scaling-foundation-models-with-lightning-attention">MiniMax-01</a> | <a href="#nvlm-open-frontier-class-multimodal-llms">NVLM</a> | <a href="#omnivlm-a-token-compressed-sub-billion-parameter-vision-language-model-for-efficient-on-device-inference">OmniVLM</a> | <a href="#pixtral-12b-a-cutting-edge-open-multimodal-language-model">Pixtral 12B</a> | <a href="#sa2va-marrying-sam2-with-llava-for-dense-grounded-understanding-of-images-and-videos">Sa2VA</a> | <a href="#tarsier2-advancing-large-vision-language-models-from-detailed-video-description-to-comprehensive-video-understanding">Tarsier2</a> | <a href="#ui-tars-pioneering-automated-gui-interaction-with-native-agents">UI-TARS</a> | <a href="#videochat-flash-hierarchical-compression-for-long-context-video-modeling">VideoChat-Flash</a> | <a href="#videollama-3-frontier-multimodal-foundation-models-for-image-and-video-understanding">VideoLLaMA 3</a> | <a href="#llama-32-vision-enhanced-multimodal-capabilities-built-on-llama-3">Llama 3.2-Vision</a> | <a href="#smolvlm-a-small-efficient-and-open-source-vision-language-model">SmolVLM</a> | <a href="#idefics">IDEFICS</a> | <a href="#idefics2">IDEFICS2</a> | <a href="#idefics3-8b-building-and-better-understanding-vision-language-models">IDEFICS3-8B</a> | <a href="#internlm-xcomposer2-mastering-free-form-text-image-composition-and-comprehension-in-vision-language-large-model">InternLM-XComposer2</a> | <a href="#internlm-xcomposer2-4khd-a-pioneering-large-vision-language-model-handling-resolutions-from-336-pixels-to-4k-hd">InternLM-XComposer2-4KHD</a> | <a href="#internlm-xcomposer-25-a-versatile-large-vision-language-model-supporting-long-contextual-input-and-output">InternLM-XComposer-2.5</a> | <a href="#internvl-25-expanding-performance-boundaries-of-open-source-multimodal-models-with-model-data-and-test-time-scaling">InternVL 2.5</a> | <a href="#deepseek-vl-towards-real-world-vision-language-understanding">DeepSeek-VL</a> | <a href="#deepseek-vl2-mixture-of-experts-vision-language-models-for-advanced-multimodal-understanding">DeepSeek-VL2</a> | <a href="#mantis-mastering-multi-image-understanding-through-interleaved-instruction-tuning">MANTIS</a> | <a href="#qwen-vl-a-versatile-vision-language-model-for-understanding-localization-text-reading-and-beyond">Qwen-VL</a> | <a href="#qwen2-vl-a-powerful-open-source-vision-language-model-for-image-and-video-understanding">Qwen2-VL</a> | <a href="#qwen25-vl-enhanced-vision-language-capabilities-in-the-qwen-series">Qwen2.5-VL</a> | <a href="#moondream1-and-moondream2">moondream1</a> | <a href="#moondream1-and-moondream2">moondream2</a> | <a href="#moondream-next-compact-vision-language-model-with-enhanced-capabilities">Moondream-next</a> | <a href="#sphinx-x-scaling-data-and-parameters-for-a-family-of-multi-modal-large-language-models">SPHINX-X</a> | <a href="#blip-bootstrapping-language-image-pre-training">BLIP</a> | <a href="#blip-2-bootstrapping-language-image-pre-training-with-frozen-image-encoders-and-large-language-models">BLIP-2</a> | <a href="#xgen-mm-blip-3-an-open-source-framework-for-building-powerful-and-responsible-large-multimodal-models">xGen-MM (BLIP-3)</a> | <a href="#instructblip-towards-general-purpose-vision-language-models-with-instruction-tuning">InstructBLIP</a> | <a href="#kosmos-1-language-is-not-all-you-need-aligning-perception-with-language-models">KOSMOS-1</a> | <a href="#kosmos-2-grounding-multimodal-large-language-models-to-the-world">KOSMOS-2</a> | <a href="#convllava-hierarchical-backbones-as-visual-encoder-for-large-multimodal-models">ConvLLaVA</a> | <a href="#parrot-multilingual-visual-instruction-tuning">Parrot</a> | <a href="#omg-llava-bridging-image-level-object-level-pixel-level-reasoning-and-understanding">OMG-LLaVA</a> | <a href="#evlm-an-efficient-vision-language-model-for-visual-understanding">EVLM</a> | <a href="#slowfast-llava-a-strong-training-free-baseline-for-video-large-language-models">SlowFast-LLaVA</a> | <a href="#nous-hermes-2-vision---mistral-7b">Nous-Hermes-2-Vision - Mistral 7B</a> | <a href="#tinygpt-v-efficient-multimodal-large-language-model-via-small-backbones">TinyGPT-V</a> | <a href="#covlm-composing-visual-entities-and-relationships-in-large-language-models-via-communicative-decoding">CoVLM</a> | <a href="#glamm-pixel-grounding-large-multimodal-model">GLaMM</a> | <a href="#cosmo-contrastive-streamlined-multimodal-model-with-interleaved-pre-training">COSMO</a> | <a href="#firellava">FireLLaVA</a> | <a href="#u-llava-unifying-multi-modal-tasks-via-large-language-model">u-LLaVA</a> | <a href="#moe-llava-mixture-of-experts-for-large-vision-language-models">MoE-LLaVA</a> | <a href="#bliva-a-simple-multimodal-llm-for-better-handling-of-text-rich-visual-questions">BLIVA</a> | <a href="#mobilevlm-a-fast-strong-and-open-vision-language-assistant-for-mobile-devices">MobileVLM</a> | <a href="#frozen-multimodal-few-shot-learning-with-frozen-language-models">FROZEN</a> | <a href="#flamingo-a-visual-language-model-for-few-shot-learning">Flamingo</a> | <a href="#openflamingo-an-open-source-framework-for-training-large-autoregressive-vision-language-models">OpenFlamingo</a> | <a href="#pali-a-jointly-scaled-multilingual-language-image-model">PaLI</a> | <a href="#pali-3-vision-language-models-smaller-faster-stronger">PaLI-3</a> | <a href="#palm-e-an-embodied-multimodal-language-model">PaLM-E</a> | <a href="#minigpt-4-enhancing-vision-language-understanding-with-advanced-large-language-models">MiniGPT-4</a> | <a href="#minigpt-v2-large-language-model-as-a-unified-interface-for-vision-language-multi-task-learning">MiniGPT-v2</a> | <a href="#llava-plus-learning-to-use-tools-for-creating-multimodal-agents">LLaVA-Plus</a> | <a href="#bakllava">BakLLaVA</a> | <a href="#cogvlm-visual-expert-for-pretrained-language-models">CogVLM</a> | <a href="#cogvlm2-enhanced-vision-language-models-for-image-and-video-understanding">CogVLM2</a> | <a href="#ferret-refer-and-ground-anything-anywhere-at-any-granularity">Ferret</a> | <a href="#fuyu-8b-a-multimodal-architecture-for-ai-agents">Fuyu-8B</a> | <a href="#otterhd-a-high-resolution-multi-modality-model">OtterHD</a> | <a href="#sphinx-the-joint-mixing-of-weights-tasks-and-visual-embeddings-for-multi-modal-large-language-models">SPHINX</a> | <a href="#eagle-2-building-post-training-data-strategies-from-scratch-for-frontier-vision-language-models">Eagle 2</a> | <a href="#eagle-exploring-the-design-space-for-multimodal-llms-with-mixture-of-encoders">EAGLE</a> | <a href="#vita-towards-open-source-interactive-omni-multimodal-llm">VITA</a> | <a href="#llava-onevision-easy-visual-task-transfer">LLaVA-OneVision</a> | <a href="#minicpm-o-26-a-gpt-4o-level-mllm-for-vision-speech-and-multimodal-live-streaming">MiniCPM-o-2.6</a> | <a href="#minicpm-v-a-gpt-4v-level-mllm-on-your-phone">MiniCPM-V</a> | <a href="#inf-llava-high-resolution-image-perception-for-multimodal-large-language-models">INF-LLaVA</a> | <a href="#florence-2-a-deep-dive-into-its-unified-architecture-and-multi-task-capabilities">Florence-2</a> | <a href="#multiinstruct-improving-multi-modal-zero-shot-learning-via-instruction-tuning">MULTIINSTRUCT</a> | <a href="#mousi-poly-visual-expert-vision-language-models">MouSi</a> | <a href="#lavin-cheap-and-quick-efficient-vision-language-instruction-tuning-for-large-language-models">LaVIN</a> | <a href="#clip-contrastive-language-image-pre-training">CLIP</a> | <a href="#metaclip-demystifying-clip-data">MetaCLIP</a> | <a href="#alpha-clip-a-clip-model-focusing-on-wherever-you-want">Alpha-CLIP</a> | <a href="#glip-grounded-language-image-pre-training">GLIP</a> | <a href="#imagebind-one-embedding-space-to-bind-them-all">ImageBind</a> | <a href="#siglip-sigmoid-loss-for-language-image-pre-training">SigLIP</a> | <a href="#vit-an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale">ViT</a> </p>
        </div>
    </article>

    <script>
        // Load full markdown dynamically for users with JS
        const markdownFile = 'vlm-architectures_README.md';

        fetch(markdownFile)
            .then(response => {
                if (!response.ok) throw new Error('File not found');
                return response.text();
            })
            .then(markdown => {
                document.getElementById('dynamic-content').innerHTML = marked.parse(markdown);
            })
            .catch(err => {
                console.error('Error loading markdown:', err);
                // Static content already visible, no action needed
            });
    </script>
</body>

</html>