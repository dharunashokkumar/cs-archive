{
    "items": [
        {
            "name": "Attention Is All You Need",
            "category": "Paper",
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "Łukasz Kaiser",
                "Illia Polosukhin"
            ],
            "year": 2017,
            "venue": "NeurIPS",
            "summary": "Introduces the Transformer, replacing recurrence with self-attention to enable parallel training and superior sequence modeling.",
            "pdf": "../data/papers/Attention_Is_All_You_Need_2017.pdf",
            "tags": [
                "transformer",
                "nlp",
                "seminal"
            ],
            "difficulty": "Advanced"
        },
        {
            "name": "BERT: Pre-training of Deep Bidirectional Transformers",
            "category": "Paper",
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "year": 2018,
            "venue": "NAACL",
            "summary": "Masked language modeling and next-sentence prediction pre-train deep bidirectional Transformers, dramatically improving NLP benchmarks.",
            "pdf": "../data/papers/BERT_Pre-training_of_Deep_Bidirectional_Transformers_2018.pdf",
            "tags": [
                "nlp",
                "pretraining"
            ],
            "difficulty": "Advanced"
        },
        {
            "name": "Language Models are Few-Shot Learners (GPT-3)",
            "category": "Paper",
            "authors": [
                "Tom B. Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "et al."
            ],
            "year": 2020,
            "venue": "NeurIPS",
            "summary": "Shows large autoregressive models can perform diverse tasks via in-context learning without gradient updates.",
            "pdf": "../data/papers/Language_Models_are_Few-Shot_Learners_(GPT-3)_2020.pdf",
            "tags": [
                "llm",
                "in-context"
            ],
            "difficulty": "Advanced"
        },
        {
            "name": "Deep Residual Learning for Image Recognition (ResNet)",
            "category": "Paper",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "year": 2015,
            "venue": "CVPR",
            "summary": "Residual connections enable very deep convolutional networks by mitigating vanishing gradients, achieving state-of-the-art on ImageNet.",
            "pdf": "../data/papers/Deep_Residual_Learning_for_Image_Recognition_(ResNet)_2015.pdf",
            "tags": [
                "cv",
                "cnn",
                "residual"
            ],
            "difficulty": "Advanced"
        },
        {
            "name": "Generative Adversarial Networks (GANs)",
            "category": "Paper",
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "year": 2014,
            "venue": "NeurIPS",
            "summary": "Introduces adversarial training: a generator and discriminator play a minimax game to produce realistic samples.",
            "pdf": "../data/papers/Generative_Adversarial_Networks_(GANs)_2014.pdf",
            "tags": [
                "generative",
                "adversarial"
            ],
            "difficulty": "Advanced"
        },
        {
            "name": "ImageNet Classification with Deep CNNs (AlexNet)",
            "category": "Paper",
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey Hinton"
            ],
            "year": 2012,
            "venue": "NeurIPS",
            "summary": "Demonstrates deep convolutional networks with ReLU, dropout, and data augmentation, igniting the deep learning revolution in vision.",
            "pdf": "../data/papers/ImageNet_Classification_with_Deep_CNNs_(AlexNet)_2012.pdf",
            "tags": [
                "cv",
                "imagenet",
                "cnn"
            ],
            "difficulty": "Advanced"
        },
        {
            "name": "You Only Look Once (YOLO)",
            "category": "Paper",
            "authors": [
                "Joseph Redmon",
                "Santosh Divvala",
                "Ross Girshick",
                "Ali Farhadi"
            ],
            "year": 2015,
            "venue": "CVPR",
            "summary": "Single-stage object detection that frames detection as regression, enabling real-time performance.",
            "pdf": "../data/papers/You_Only_Look_Once_(YOLO)_2015.pdf",
            "tags": [
                "cv",
                "detection",
                "real-time"
            ],
            "difficulty": "Advanced"
        },
        {
            "name": "Mastering the game of Go with deep neural networks and tree search (AlphaGo)",
            "category": "Paper",
            "authors": [
                "David Silver",
                "Aja Huang",
                "Chris Maddison",
                "et al."
            ],
            "year": 2016,
            "venue": "Nature",
            "summary": "Combines policy/value networks with Monte Carlo tree search to surpass human performance in Go.",
            "pdf": "../data/papers/alphago-silver-2016.pdf",
            "tags": [
                "rl",
                "deepmind",
                "mcts"
            ],
            "difficulty": "Advanced"
        },
        {
            "name": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
            "category": "Paper",
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "et al."
            ],
            "year": 2021,
            "venue": "ICML",
            "summary": "Pre-trains vision encoders on image–text pairs to enable zero-shot recognition and flexible multimodal tasks.",
            "pdf": "../data/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision_(CLIP)_2021.pdf",
            "tags": [
                "multimodal",
                "zero-shot"
            ],
            "difficulty": "Advanced"
        },
        {
            "name": "NeRF: Representing Scenes as Neural Radiance Fields",
            "category": "Paper",
            "authors": [
                "Ben Mildenhall",
                "Pratul Srinivasan",
                "Matthew Tancik",
                "et al."
            ],
            "year": 2020,
            "venue": "ECCV",
            "summary": "Optimizes a radiance field with volumetric rendering to synthesize novel views from sparse images.",
            "pdf": "../data/papers/NeRF_Representing_Scenes_as_Neural_Radiance_Fields_2020.pdf",
            "tags": [
                "3d",
                "vision",
                "rendering"
            ],
            "difficulty": "Advanced"
        },
        {
            "name": "Chain-of-Thought Prompting Elicits Reasoning in LLMs",
            "category": "Paper",
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "et al."
            ],
            "year": 2022,
            "venue": "NeurIPS",
            "summary": "Simple prompting that encourages step-by-step reasoning improves performance on arithmetic and logic tasks.",
            "pdf": "../data/papers/Chain-of-Thought_Prompting_Elicits_Reasoning_in_LLMs_2022.pdf",
            "tags": [
                "prompting",
                "reasoning"
            ],
            "difficulty": "Intermediate"
        },
        {
            "name": "FlashAttention: Fast and Memory-Efficient Exact Attention",
            "category": "Paper",
            "authors": [
                "Tri Dao",
                "Daniel Y. Fu",
                "Stefano Ermon",
                "Atri Rudra"
            ],
            "year": 2022,
            "venue": "NeurIPS",
            "summary": "IO-aware attention algorithm that tiles queries/keys to maximize GPU bandwidth and reduce memory reads.",
            "pdf": "../data/papers/FlashAttention_Fast_and_Memory-Efficient_Exact_Attention_2022.pdf",
            "tags": [
                "optimization",
                "efficient"
            ],
            "difficulty": "Advanced"
        }
    ]
}
